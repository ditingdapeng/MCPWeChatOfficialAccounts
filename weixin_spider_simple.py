#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬è™« - å‚è€ƒpythonSpiderå®ç°çš„å®Œæ•´ç‰ˆæœ¬
æ”¯æŒæ–‡ç« å†…å®¹æŠ“å–ã€å›¾ç‰‡ä¸‹è½½ã€å¤šç§æ ¼å¼ä¿å­˜
"""

import time
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import json
import os
import logging
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse
import hashlib
from pathlib import Path
import sys
import shutil
import subprocess
import base64
from urllib.parse import unquote

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class WeixinSpiderWithImages:
    def __init__(self, headless=True, wait_time=10, download_images=True):
        """
        åˆå§‹åŒ–çˆ¬è™«
        :param headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
        :param wait_time: é¡µé¢ç­‰å¾…æ—¶é—´
        :param download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡
        """
        self.driver = None
        self.wait_time = wait_time
        self.download_images = download_images
        self.session = requests.Session()
        self.setup_session()
        self.setup_driver(headless)
        
    def setup_session(self):
        """è®¾ç½®requestsä¼šè¯"""
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
    def find_chromedriver_path(self):
        """æŸ¥æ‰¾ç³»ç»Ÿä¸­çš„ChromeDriverè·¯å¾„"""
        # å¸¸è§çš„ChromeDriverè·¯å¾„
        possible_paths = [
            '/usr/local/bin/chromedriver',
            '/usr/bin/chromedriver',
            '/opt/homebrew/bin/chromedriver',
            shutil.which('chromedriver'),
        ]
        
        for path in possible_paths:
            if path and os.path.exists(path) and os.access(path, os.X_OK):
                logger.info(f"æ‰¾åˆ°ChromeDriver: {path}")
                return path
        
        # å°è¯•é€šè¿‡brewå®‰è£…
        try:
            result = subprocess.run(['brew', '--version'], capture_output=True, text=True)
            if result.returncode == 0:
                logger.info("æ£€æµ‹åˆ°Homebrewï¼Œå°è¯•å®‰è£…ChromeDriver...")
                subprocess.run(['brew', 'install', 'chromedriver'], check=True)
                chromedriver_path = shutil.which('chromedriver')
                if chromedriver_path:
                    return chromedriver_path
        except Exception as e:
            logger.warning(f"é€šè¿‡Homebrewå®‰è£…ChromeDriverå¤±è´¥: {e}")
        
        return None
        
    def setup_driver(self, headless=True):
        """è®¾ç½®Chromeæµè§ˆå™¨é©±åŠ¨"""
        try:
            logger.info("æ­£åœ¨è®¾ç½®Chromeæµè§ˆå™¨é©±åŠ¨...")
            
            options = Options()
            
            if headless:
                options.add_argument('--headless')  # ä½¿ç”¨ä¼ ç»Ÿheadlessæ¨¡å¼
                logger.info("ä½¿ç”¨æ— å¤´æ¨¡å¼")
            
            # åŸºæœ¬è®¾ç½® - é’ˆå¯¹ç‰ˆæœ¬å…¼å®¹æ€§å’Œæ¸²æŸ“å™¨è¿æ¥é—®é¢˜ä¼˜åŒ–
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--disable-extensions')
            options.add_argument('--disable-logging')
            options.add_argument('--disable-web-security')
            options.add_argument('--allow-running-insecure-content')
            options.add_argument('--disable-features=VizDisplayCompositor')
            options.add_argument('--disable-features=TranslateUI')
            options.add_argument('--disable-background-timer-throttling')
            options.add_argument('--disable-backgrounding-occluded-windows')
            options.add_argument('--disable-renderer-backgrounding')
            options.add_argument('--disable-ipc-flooding-protection')
            options.add_argument('--disable-hang-monitor')
            options.add_argument('--disable-client-side-phishing-detection')
            options.add_argument('--disable-popup-blocking')
            options.add_argument('--disable-prompt-on-repost')
            options.add_argument('--disable-sync')
            options.add_argument('--no-first-run')
            options.add_argument('--disable-default-apps')
            options.add_argument('--disable-component-update')
            options.add_argument('--disable-background-networking')
            options.add_argument('--disable-component-extensions-with-background-pages')
            options.add_argument('--disable-blink-features=AutomationControlled')
            
            # è§£å†³æ¸²æŸ“å™¨è¿æ¥é—®é¢˜çš„å…³é”®å‚æ•°
            options.add_argument('--single-process')  # ä½¿ç”¨å•è¿›ç¨‹æ¨¡å¼
            options.add_argument('--disable-gpu-sandbox')
            options.add_argument('--disable-software-rasterizer')
            options.add_argument('--remote-debugging-port=0')  # ç¦ç”¨è¿œç¨‹è°ƒè¯•ç«¯å£
            options.add_argument('--disable-dev-tools')
            
            # å†…å­˜å’Œæ€§èƒ½ä¼˜åŒ–
            options.add_argument('--memory-pressure-off')
            options.add_argument('--max_old_space_size=4096')
            
            # è®¾ç½®ç”¨æˆ·ä»£ç† - æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬
            options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36')
            
            # æ’é™¤è‡ªåŠ¨åŒ–æ ‡è¯†
            options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
            options.add_experimental_option('useAutomationExtension', False)
            
            # è®¾ç½®prefsä»¥é¿å…å„ç§å¼¹çª—
            prefs = {
                "profile.default_content_setting_values": {
                    "notifications": 2,
                    "geolocation": 2,
                    "media_stream": 2,
                },
                "profile.default_content_settings.popups": 0,
                "profile.managed_default_content_settings.images": 2 if not self.download_images else 1
            }
            options.add_experimental_option("prefs", prefs)
            
            # ä¼˜å…ˆå°è¯•ä½¿ç”¨ç³»ç»ŸChromeDriver
            try:
                logger.info("å°è¯•ä½¿ç”¨ç³»ç»ŸChromeDriver...")
                # æ£€æŸ¥ChromeDriverè·¯å¾„
                chromedriver_path = shutil.which('chromedriver')
                if chromedriver_path:
                    logger.info(f"æ‰¾åˆ°ChromeDriverè·¯å¾„: {chromedriver_path}")
                    service = Service(chromedriver_path)
                    self.driver = webdriver.Chrome(service=service, options=options)
                    logger.info("ä½¿ç”¨ç³»ç»ŸChromeDriveræˆåŠŸåˆå§‹åŒ–")
                else:
                    raise Exception("æœªæ‰¾åˆ°ç³»ç»ŸChromeDriver")
            except Exception as system_error:
                logger.warning(f"ç³»ç»ŸChromeDriverå¤±è´¥: {system_error}")
                try:
                    logger.info("ä½¿ç”¨webdriver-managerè‡ªåŠ¨ä¸‹è½½å…¼å®¹çš„ChromeDriver...")
                    from webdriver_manager.chrome import ChromeDriverManager
                    service = Service(ChromeDriverManager().install())
                    self.driver = webdriver.Chrome(service=service, options=options)
                    logger.info("ä½¿ç”¨webdriver-manageræˆåŠŸåˆå§‹åŒ–ChromeDriver")
                except Exception as wdm_error:
                    logger.error(f"webdriver-managerå¤±è´¥: {wdm_error}")
                    # æœ€åå°è¯•ä¸æŒ‡å®šservice
                    try:
                        logger.info("å°è¯•ä½¿ç”¨é»˜è®¤ChromeDriveré…ç½®...")
                        self.driver = webdriver.Chrome(options=options)
                        logger.info("ä½¿ç”¨é»˜è®¤é…ç½®æˆåŠŸåˆå§‹åŒ–ChromeDriver")
                    except Exception as default_error:
                        logger.error(f"é»˜è®¤é…ç½®ä¹Ÿå¤±è´¥: {default_error}")
                        raise RuntimeError(f"æ‰€æœ‰ChromeDriveråˆå§‹åŒ–æ–¹æ³•éƒ½å¤±è´¥: {default_error}")
            
            # æ‰§è¡Œè„šæœ¬éšè—webdriverå±æ€§
            try:
                self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                self.driver.execute_script("Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})")
                self.driver.execute_script("Object.defineProperty(navigator, 'languages', {get: () => ['zh-CN', 'zh', 'en']})")
            except Exception as script_error:
                logger.warning(f"æ‰§è¡Œéšè—è„šæœ¬å¤±è´¥: {script_error}")
            
            # è®¾ç½®çª—å£å¤§å°
            try:
                self.driver.set_window_size(1920, 1080)
            except Exception as window_error:
                logger.warning(f"è®¾ç½®çª—å£å¤§å°å¤±è´¥: {window_error}")
            
            logger.info("Chromeæµè§ˆå™¨é©±åŠ¨è®¾ç½®å®Œæˆ")
            
        except Exception as e:
            logger.error(f"è®¾ç½®æµè§ˆå™¨é©±åŠ¨å¤±è´¥: {str(e)}")
            # æä¾›å¤‡ç”¨æ–¹æ¡ˆ
            self.driver = None
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–Chromeæµè§ˆå™¨é©±åŠ¨: {e}")
    
    def crawl_article_by_url(self, url, retry_times=3):
        """é€šè¿‡URLæŠ“å–æ–‡ç« å†…å®¹ï¼Œæ”¯æŒé‡è¯•"""
        if not self.driver:
            raise RuntimeError("æµè§ˆå™¨é©±åŠ¨æœªåˆå§‹åŒ–")
            
        for attempt in range(retry_times):
            try:
                logger.info(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•è®¿é—®æ–‡ç« : {url}")
                
                # è®¿é—®é¡µé¢
                self.driver.get(url)
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                wait = WebDriverWait(self.driver, self.wait_time)
                
                # ç­‰å¾…æ–‡ç« æ ‡é¢˜åŠ è½½
                try:
                    title_element = wait.until(
                        EC.presence_of_element_located((By.ID, "activity-name"))
                    )
                except:
                    # å°è¯•å…¶ä»–å¯èƒ½çš„æ ‡é¢˜å…ƒç´ 
                    title_element = wait.until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "h1, .rich_media_title, #js_title"))
                    )
                
                # æ»šåŠ¨é¡µé¢ç¡®ä¿å†…å®¹å®Œå…¨åŠ è½½
                self._scroll_page()
                
                # æå–æ–‡ç« ä¿¡æ¯
                article_data = self._extract_article_content()
                
                if article_data and article_data.get('title'):
                    logger.info(f"æˆåŠŸæŠ“å–æ–‡ç« : {article_data['title']}")
                    return article_data
                else:
                    logger.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•æœªèƒ½è·å–å®Œæ•´æ–‡ç« å†…å®¹")
                    
            except Exception as e:
                logger.error(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {str(e)}")
                if attempt == retry_times - 1:
                    raise
                time.sleep(2)
        
        raise Exception("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
    
    def _scroll_page(self):
        """æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ‰€æœ‰å†…å®¹"""
        try:
            # è·å–é¡µé¢é«˜åº¦
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            
            while True:
                # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                
                # ç­‰å¾…æ–°å†…å®¹åŠ è½½
                time.sleep(2)
                
                # è®¡ç®—æ–°çš„é¡µé¢é«˜åº¦
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                
                if new_height == last_height:
                    break
                    
                last_height = new_height
                
            # æ»šåŠ¨å›é¡¶éƒ¨
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)
            
        except Exception as e:
            logger.warning(f"æ»šåŠ¨é¡µé¢æ—¶å‡ºé”™: {e}")
    
    def _extract_article_content(self):
        """æå–æ–‡ç« å†…å®¹"""
        try:
            article_data = {}
            
            # è·å–æ–‡ç« æ ‡é¢˜ - å°è¯•å¤šç§é€‰æ‹©å™¨
            title_selectors = [
                "#activity-name",
                ".rich_media_title",
                "#js_title",
                "h1",
                "[class*='title']"
            ]
            
            title = self._get_text_by_selectors(title_selectors, "æœªçŸ¥æ ‡é¢˜")
            article_data['title'] = title.strip()
            logger.info(f"æå–åˆ°æ ‡é¢˜: {title}")
            
            # è·å–ä½œè€…ä¿¡æ¯
            author_selectors = [
                "#js_name",
                ".rich_media_meta_text",
                "[class*='author']",
                "[id*='author']"
            ]
            author = self._get_text_by_selectors(author_selectors, "æœªçŸ¥ä½œè€…")
            article_data['author'] = author.strip()
            
            # è·å–å‘å¸ƒæ—¶é—´
            time_selectors = [
                "#publish_time",
                ".rich_media_meta_text",
                "[class*='time']",
                "[id*='time']"
            ]
            publish_time = self._get_text_by_selectors(time_selectors, "æœªçŸ¥æ—¶é—´")
            article_data['publish_time'] = publish_time.strip()
            
            # è·å–æ–‡ç« æ­£æ–‡å†…å®¹
            content_selectors = [
                "#js_content",
                ".rich_media_content",
                "[class*='content']",
                "article"
            ]
            
            content_element = self._get_element_by_selectors(content_selectors)
            
            if content_element:
                # è·å–HTMLå†…å®¹
                content_html = content_element.get_attribute('innerHTML')
                article_data['content_html'] = content_html
                
                # æå–å›¾ç‰‡ä¿¡æ¯
                if self.download_images:
                    images_info = self._extract_images_from_content(content_element)
                    article_data['images'] = images_info
                    logger.info(f"å‘ç° {len(images_info)} å¼ å›¾ç‰‡")
                
                # ä½¿ç”¨BeautifulSoupè§£æHTMLï¼Œæå–çº¯æ–‡æœ¬
                soup = BeautifulSoup(content_html, 'html.parser')
                
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                for script in soup(["script", "style"]):
                    script.decompose()
                
                content_text = soup.get_text(separator='\n', strip=True)
                article_data['content_text'] = content_text
                
                logger.info(f"æå–åˆ°å†…å®¹é•¿åº¦: {len(content_text)} å­—ç¬¦")
            else:
                logger.warning("æœªèƒ½æ‰¾åˆ°æ–‡ç« æ­£æ–‡å†…å®¹")
                article_data['content_html'] = ""
                article_data['content_text'] = ""
                article_data['images'] = []
            
            # è·å–å½“å‰URL
            article_data['url'] = self.driver.current_url
            article_data['crawl_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            return article_data
            
        except Exception as e:
            logger.error(f"æå–æ–‡ç« å†…å®¹å¤±è´¥: {str(e)}")
            return None
    
    def _extract_images_from_content(self, content_element):
        """ä»å†…å®¹ä¸­æå–å›¾ç‰‡ä¿¡æ¯"""
        images_info = []
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡å…ƒç´ 
            img_elements = content_element.find_elements(By.TAG_NAME, "img")
            
            for i, img in enumerate(img_elements):
                try:
                    # è·å–å›¾ç‰‡URL - ä¼˜å…ˆä½¿ç”¨data-srcï¼ˆå¾®ä¿¡æ–‡ç« çš„çœŸå®å›¾ç‰‡é“¾æ¥ï¼‰
                    img_url = img.get_attribute('data-src') or img.get_attribute('src')
                    
                    if not img_url:
                        continue
                    
                    # å¤„ç†ç›¸å¯¹URL
                    if img_url.startswith('//'):
                        img_url = 'https:' + img_url
                    elif img_url.startswith('/'):
                        img_url = urljoin(self.driver.current_url, img_url)
                    
                    # è·å–å›¾ç‰‡å…¶ä»–å±æ€§
                    alt_text = img.get_attribute('alt') or f"å›¾ç‰‡_{i+1}"
                    title_text = img.get_attribute('title') or ""
                    
                    image_info = {
                        'index': i + 1,
                        'url': img_url,
                        'alt': alt_text,
                        'title': title_text,
                        'filename': None,  # å°†åœ¨ä¸‹è½½æ—¶è®¾ç½®
                        'local_path': None,  # å°†åœ¨ä¸‹è½½æ—¶è®¾ç½®
                        'download_success': False
                    }
                    
                    images_info.append(image_info)
                    
                except Exception as e:
                    logger.warning(f"å¤„ç†ç¬¬ {i+1} å¼ å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
                    continue
            
        except Exception as e:
            logger.error(f"æå–å›¾ç‰‡ä¿¡æ¯å¤±è´¥: {str(e)}")
        
        return images_info
    
    def _get_text_by_selectors(self, selectors, default=""):
        """é€šè¿‡å¤šä¸ªé€‰æ‹©å™¨å°è¯•è·å–æ–‡æœ¬"""
        for selector in selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                text = element.text.strip()
                if text:
                    return text
            except:
                continue
        return default
    
    def _get_element_by_selectors(self, selectors):
        """é€šè¿‡å¤šä¸ªé€‰æ‹©å™¨å°è¯•è·å–å…ƒç´ """
        for selector in selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                if element:
                    return element
            except:
                continue
        return None
    
    def _download_image(self, img_url, save_dir, filename_prefix="img"):
        """ä¸‹è½½å•å¼ å›¾ç‰‡å¹¶è½¬æ¢ä¸ºPNGæ ¼å¼"""
        try:
            # å¤„ç† data: URL (å†…è”å›¾ç‰‡)
            if img_url.startswith('data:'):
                return self._save_data_url_image_as_png(img_url, save_dir, filename_prefix)
            
            # å‘é€è¯·æ±‚ä¸‹è½½å›¾ç‰‡
            response = self.session.get(img_url, timeout=30, stream=True)
            response.raise_for_status()
            
            # ç”ŸæˆPNGæ–‡ä»¶åï¼ˆä½¿ç”¨å‰ç¼€ï¼Œç¡®ä¿å”¯ä¸€æ€§ï¼‰
            filename = f"{filename_prefix}.png"
            filepath = os.path.join(save_dir, filename)
            
            # å…ˆä¿å­˜åŸå§‹æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
            temp_filepath = filepath + ".temp"
            with open(temp_filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # è½¬æ¢ä¸ºPNGæ ¼å¼
            try:
                from PIL import Image
                # æ‰“å¼€å›¾ç‰‡å¹¶è½¬æ¢ä¸ºPNG
                with Image.open(temp_filepath) as img:
                    # å¦‚æœæ˜¯RGBAæ¨¡å¼ï¼Œä¿æŒé€æ˜åº¦ï¼›å¦åˆ™è½¬æ¢ä¸ºRGB
                    if img.mode in ('RGBA', 'LA', 'P'):
                        img = img.convert('RGBA')
                    else:
                        img = img.convert('RGB')
                    img.save(filepath, 'PNG')
                
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                os.remove(temp_filepath)
                
                logger.info(f"å›¾ç‰‡ä¸‹è½½å¹¶è½¬æ¢ä¸ºPNGæˆåŠŸ: {filename}")
                return filename, filepath
                
            except Exception as convert_error:
                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿ç•™åŸå§‹æ–‡ä»¶ä½†é‡å‘½åä¸ºPNG
                logger.warning(f"å›¾ç‰‡è½¬æ¢å¤±è´¥ï¼Œä¿å­˜åŸå§‹æ–‡ä»¶: {str(convert_error)}")
                os.rename(temp_filepath, filepath)
                return filename, filepath
            
        except Exception as e:
            logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {img_url}: {str(e)}")
            return None, None
    
    def _save_data_url_image_as_png(self, data_url, save_dir, filename_prefix="img"):
        """ä¿å­˜ data: URL æ ¼å¼çš„å†…è”å›¾ç‰‡å¹¶è½¬æ¢ä¸ºPNG"""
        try:
            # è§£æ data URL
            if not data_url.startswith('data:'):
                return None, None
            
            # ç§»é™¤ 'data:' å‰ç¼€
            data_part = data_url[5:]
            
            # åˆ†ç¦»åª’ä½“ç±»å‹å’Œæ•°æ®
            if ',' not in data_part:
                return None, None
            
            header, data = data_part.split(',', 1)
            
            # è§£æåª’ä½“ç±»å‹å’Œç¼–ç 
            if ';base64' in header:
                # Base64 ç¼–ç çš„æ•°æ®
                media_type = header.replace(';base64', '')
                try:
                    image_data = base64.b64decode(data)
                except Exception:
                    logger.warning(f"Base64è§£ç å¤±è´¥: {data_url[:100]}...")
                    return None, None
            else:
                # URLç¼–ç çš„æ•°æ® (å¦‚SVG)
                media_type = header
                try:
                    image_data = unquote(data).encode('utf-8')
                except Exception:
                    logger.warning(f"URLè§£ç å¤±è´¥: {data_url[:100]}...")
                    return None, None
            
            # ç”ŸæˆPNGæ–‡ä»¶å
            filename = f"{filename_prefix}.png"
            filepath = os.path.join(save_dir, filename)
            
            # å¤„ç†ä¸åŒæ ¼å¼
            try:
                from PIL import Image
                import io
                
                # ä»å­—èŠ‚æ•°æ®åˆ›å»ºå›¾ç‰‡
                img = Image.open(io.BytesIO(image_data))
                
                # è½¬æ¢ä¸ºPNGæ ¼å¼
                if img.mode in ('RGBA', 'LA', 'P'):
                    img = img.convert('RGBA')
                else:
                    img = img.convert('RGB')
                
                img.save(filepath, 'PNG')
                logger.info(f"å†…è”å›¾ç‰‡è½¬æ¢ä¸ºPNGæˆåŠŸ: {filename}")
                
            except Exception:
                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œç›´æ¥ä¿å­˜åŸå§‹æ•°æ®
                with open(filepath, 'wb') as f:
                    f.write(image_data)
                logger.info(f"å†…è”å›¾ç‰‡ä¿å­˜ä¸ºPNGæ–‡ä»¶å: {filename}")
            
            return filename, filepath
            
        except Exception as e:
            logger.error(f"ä¿å­˜å†…è”å›¾ç‰‡å¤±è´¥: {str(e)}")
            return None, None
    
    def _download_all_images(self, images_info, save_dir):
        """ä¸‹è½½æ‰€æœ‰å›¾ç‰‡"""
        if not images_info:
            return
        
        # åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•
        images_dir = os.path.join(save_dir, "images")
        os.makedirs(images_dir, exist_ok=True)
        
        logger.info(f"å¼€å§‹ä¸‹è½½ {len(images_info)} å¼ å›¾ç‰‡...")
        
        for img_info in images_info:
            try:
                filename, filepath = self._download_image(
                    img_info['url'], 
                    images_dir, 
                    f"img_{img_info['index']:03d}"
                )
                
                if filename and filepath:
                    img_info['filename'] = filename
                    img_info['local_path'] = filepath
                    img_info['download_success'] = True
                else:
                    img_info['download_success'] = False
                
                # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"ä¸‹è½½å›¾ç‰‡ {img_info['url']} æ—¶å‡ºé”™: {str(e)}")
                img_info['download_success'] = False
        
        success_count = sum(1 for img in images_info if img['download_success'])
        logger.info(f"å›¾ç‰‡ä¸‹è½½å®Œæˆ: {success_count}/{len(images_info)} å¼ æˆåŠŸ")
    
    def save_article_to_file(self, article_data, custom_filename=None):
        """ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶"""
        if not article_data:
            logger.warning("æ²¡æœ‰æ–‡ç« æ•°æ®å¯ä¿å­˜")
            return False
        
        try:
            # åˆ›å»ºä¿å­˜ç›®å½•
            save_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "articles")
            os.makedirs(save_dir, exist_ok=True)
            
            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            if custom_filename:
                safe_filename = custom_filename
            else:
                title = article_data.get('title', 'æœªçŸ¥æ ‡é¢˜')
                # ç§»é™¤æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
                safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)
                safe_title = safe_title[:50]  # é™åˆ¶é•¿åº¦
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                safe_filename = f"{safe_title}_{timestamp}"
            
            # åˆ›å»ºæ–‡ç« ä¸“ç”¨ç›®å½•
            article_dir = os.path.join(save_dir, safe_filename)
            os.makedirs(article_dir, exist_ok=True)
            
            # ä¸‹è½½å›¾ç‰‡
            if self.download_images and article_data.get('images'):
                self._download_all_images(article_data['images'], article_dir)
            
            # ä¿å­˜JSONæ ¼å¼
            json_filepath = os.path.join(article_dir, f"{safe_filename}.json")
            with open(json_filepath, 'w', encoding='utf-8') as f:
                json.dump(article_data, f, ensure_ascii=False, indent=2)
            logger.info(f"JSONæ–‡ä»¶å·²ä¿å­˜: {json_filepath}")
            
            # ä¿å­˜TXTæ ¼å¼
            txt_filepath = os.path.join(article_dir, f"{safe_filename}.txt")
            with open(txt_filepath, 'w', encoding='utf-8') as f:
                f.write(f"æ ‡é¢˜: {article_data.get('title', '')}\n")
                f.write(f"ä½œè€…: {article_data.get('author', '')}\n")
                f.write(f"å‘å¸ƒæ—¶é—´: {article_data.get('publish_time', '')}\n")
                f.write(f"æŠ“å–æ—¶é—´: {article_data.get('crawl_time', '')}\n")
                f.write(f"é“¾æ¥: {article_data.get('url', '')}\n")
                f.write("\n" + "="*80 + "\n\n")
                f.write(article_data.get('content_text', ''))
                
                # æ·»åŠ å›¾ç‰‡ä¿¡æ¯
                if article_data.get('images'):
                    f.write("\n\n" + "="*80 + "\n")
                    f.write("å›¾ç‰‡ä¿¡æ¯:\n")
                    for img in article_data['images']:
                        f.write(f"\nå›¾ç‰‡ {img['index']}: {img['alt']}\n")
                        f.write(f"åŸå§‹URL: {img['url']}\n")
                        if img['download_success']:
                            f.write(f"æœ¬åœ°æ–‡ä»¶: {img['filename']}\n")
                        else:
                            f.write("ä¸‹è½½å¤±è´¥\n")
            
            logger.info(f"TXTæ–‡ä»¶å·²ä¿å­˜: {txt_filepath}")
            
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False
    
    def close(self):
        """å…³é—­æµè§ˆå™¨å’Œä¼šè¯"""
        if self.driver:
            try:
                self.driver.quit()
                logger.info("æµè§ˆå™¨å·²å…³é—­")
            except Exception as e:
                logger.error(f"å…³é—­æµè§ˆå™¨æ—¶å‡ºé”™: {str(e)}")
        
        if self.session:
            try:
                self.session.close()
                logger.info("ç½‘ç»œä¼šè¯å·²å…³é—­")
            except Exception as e:
                logger.error(f"å…³é—­ç½‘ç»œä¼šè¯æ—¶å‡ºé”™: {str(e)}")
    
    def __del__(self):
        """ææ„å‡½æ•° - ä¸è‡ªåŠ¨å…³é—­é©±åŠ¨ï¼Œé¿å…æ„å¤–å…³é—­"""
        # æ³¨é‡Šæ‰è‡ªåŠ¨å…³é—­ï¼Œé¿å…åœ¨ä¸åˆé€‚çš„æ—¶æœºå…³é—­é©±åŠ¨
        # self.close()
        pass

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åŒ…æ˜¯å¦å®‰è£…"""
    required_packages = {
        'selenium': 'selenium',
        'beautifulsoup4': 'bs4',
        'requests': 'requests',
        'webdriver_manager': 'webdriver_manager',
        'Pillow': 'PIL'
    }
    
    missing_packages = []
    
    for package_name, import_name in required_packages.items():
        try:
            __import__(import_name)
            logger.info(f"âœ… {package_name} å·²å®‰è£…")
        except ImportError:
            missing_packages.append(package_name)
            logger.error(f"âŒ {package_name} æœªå®‰è£…")
    
    if missing_packages:
        logger.error(f"\nç¼ºå°‘ä»¥ä¸‹ä¾èµ–åŒ…: {', '.join(missing_packages)}")
        logger.error("è¯·è¿è¡Œ: pip install selenium beautifulsoup4 requests webdriver-manager Pillow")
        return False
    
    logger.info("âœ… æ‰€æœ‰ä¾èµ–åŒ…éƒ½å·²å®‰è£…")
    return True


def test_spider_with_images():
    """æµ‹è¯•çˆ¬è™«åŠŸèƒ½ï¼ˆåŒ…å«å›¾ç‰‡ä¸‹è½½ï¼‰"""
    # æµ‹è¯•URLåˆ—è¡¨
    test_urls = [
        "https://mp.weixin.qq.com/s/KJl2oTMaKRra2l0PV7IIiA",  # ç›®æ ‡æ–‡ç« 
        # å¯ä»¥æ·»åŠ æ›´å¤šæµ‹è¯•URL
    ]
    
    spider = None
    
    try:
        logger.info("å¼€å§‹æµ‹è¯•å¾®ä¿¡å…¬ä¼—å·çˆ¬è™«ï¼ˆåŒ…å«å›¾ç‰‡ä¸‹è½½ï¼‰...")
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        spider = WeixinSpiderWithImages(headless=False, wait_time=10, download_images=True)
        
        for i, url in enumerate(test_urls, 1):
            logger.info(f"\n=== æµ‹è¯•ç¬¬ {i} ä¸ªURL ===")
            logger.info(f"URL: {url}")
            
            # æŠ“å–æ–‡ç« 
            article_data = spider.crawl_article_by_url(url)
            
            if article_data:
                logger.info("âœ… æŠ“å–æˆåŠŸ")
                logger.info(f"æ ‡é¢˜: {article_data.get('title', 'N/A')}")
                logger.info(f"ä½œè€…: {article_data.get('author', 'N/A')}")
                logger.info(f"å‘å¸ƒæ—¶é—´: {article_data.get('publish_time', 'N/A')}")
                logger.info(f"å†…å®¹é•¿åº¦: {len(article_data.get('content_text', ''))} å­—ç¬¦")
                logger.info(f"å›¾ç‰‡æ•°é‡: {len(article_data.get('images', []))} å¼ ")
                
                # æ˜¾ç¤ºå›¾ç‰‡ä¿¡æ¯
                images = article_data.get('images', [])
                if images:
                    logger.info("\nğŸ“· å›¾ç‰‡ä¿¡æ¯:")
                    for img in images:
                        logger.info(f"  å›¾ç‰‡ {img['index']}: {img['alt']}")
                        logger.info(f"    URL: {img['url'][:80]}...")
                
                # ä¿å­˜æ–‡ç« 
                if spider.save_article_to_file(article_data, f"test_with_images_{i}"):
                    logger.info("âœ… æ–‡ç« å’Œå›¾ç‰‡ä¿å­˜æˆåŠŸ")
                    
                    # ç»Ÿè®¡ä¸‹è½½æˆåŠŸçš„å›¾ç‰‡
                    if images:
                        success_count = sum(1 for img in images if img.get('download_success', False))
                        logger.info(f"ğŸ“Š å›¾ç‰‡ä¸‹è½½ç»Ÿè®¡: {success_count}/{len(images)} å¼ æˆåŠŸ")
                else:
                    logger.error("âŒ æ–‡ç« ä¿å­˜å¤±è´¥")
            else:
                logger.error("âŒ æŠ“å–å¤±è´¥")
            
            # å¦‚æœæœ‰å¤šä¸ªURLï¼Œæ·»åŠ å»¶æ—¶
            if i < len(test_urls):
                logger.info("ç­‰å¾…5ç§’åç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")
                time.sleep(5)
        
        logger.info("\n=== æµ‹è¯•å®Œæˆ ===")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
    finally:
        if spider:
            spider.close()


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=== å¾®ä¿¡å…¬ä¼—å·çˆ¬è™«æµ‹è¯•ç¨‹åºï¼ˆæ”¯æŒå›¾ç‰‡ä¸‹è½½ï¼‰===")
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        sys.exit(1)
    
    # è¿è¡Œæµ‹è¯•
    test_spider_with_images()


if __name__ == "__main__":
    main()