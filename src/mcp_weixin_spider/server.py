#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MCPå¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬è™«æœåŠ¡å™¨

æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
1. çˆ¬å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹
2. ä¸‹è½½æ–‡ç« ä¸­çš„å›¾ç‰‡
3. è¿”å›ç»“æ„åŒ–çš„æ–‡ç« æ•°æ®
4. æä¾›æ–‡ç« å†…å®¹åˆ†æ
"""

import asyncio
import json
import logging
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional

from mcp.server import Server
from mcp.server.models import InitializationOptions
from mcp.server.models import ServerCapabilities
from mcp.server.stdio import stdio_server
from mcp.types import (
    Resource,
    Tool,
    TextContent,
    ImageContent,
    EmbeddedResource,
)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
import os
project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
sys.path.append(project_root)

try:
    # ä¼˜å…ˆä½¿ç”¨ç®€åŒ–ç‰ˆçˆ¬è™«
    from weixin_spider_simple import WeixinSpiderWithImages
    logging.info("ä½¿ç”¨ç®€åŒ–ç‰ˆçˆ¬è™«æ¨¡å—")
except ImportError:
    try:
        # å¤‡ç”¨ï¼šä½¿ç”¨åŸç‰ˆçˆ¬è™«
        python_spider_path = os.path.join(project_root, 'pythonSpider')
        sys.path.append(python_spider_path)
        from weixin_spider_with_image import WeixinSpiderWithImages
        logging.info("ä½¿ç”¨åŸç‰ˆçˆ¬è™«æ¨¡å—")
    except ImportError as e:
        logging.error(f"å¯¼å…¥çˆ¬è™«æ¨¡å—å¤±è´¥: {e}")
        logging.error(f"Pythonè·¯å¾„: {sys.path}")
        logging.error(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        logging.error(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
        WeixinSpiderWithImages = None

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åˆ›å»ºMCPæœåŠ¡å™¨å®ä¾‹
server = Server("mcp-weixin-spider")

# å…¨å±€çˆ¬è™«å®ä¾‹
spider_instance: Optional[WeixinSpiderWithImages] = None


def get_spider_instance() -> WeixinSpiderWithImages:
    """è·å–çˆ¬è™«å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global spider_instance
    if spider_instance is None:
        if WeixinSpiderWithImages is None:
            raise RuntimeError("çˆ¬è™«æ¨¡å—æœªæ­£ç¡®å¯¼å…¥")
        try:
            spider_instance = WeixinSpiderWithImages(
                headless=True,  # MCPæœåŠ¡å™¨ä¸­ä½¿ç”¨æ— å¤´æ¨¡å¼
                wait_time=10,
                download_images=True
            )
            logger.info("çˆ¬è™«å®ä¾‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"çˆ¬è™«å®ä¾‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–çˆ¬è™«å®ä¾‹: {e}")
    return spider_instance


@server.list_tools()
async def handle_list_tools() -> List[Tool]:
    """åˆ—å‡ºå¯ç”¨çš„å·¥å…·"""
    return [
        Tool(
            name="crawl_weixin_article",
            description="çˆ¬å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹å’Œå›¾ç‰‡",
            inputSchema={
                "type": "object",
                "properties": {
                    "url": {
                        "type": "string",
                        "description": "å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çš„URLé“¾æ¥"
                    },
                    "download_images": {
                        "type": "boolean",
                        "description": "æ˜¯å¦ä¸‹è½½æ–‡ç« ä¸­çš„å›¾ç‰‡",
                        "default": True
                    },
                    "custom_filename": {
                        "type": "string",
                        "description": "è‡ªå®šä¹‰æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰",
                        "default": None
                    }
                },
                "required": ["url"]
            }
        ),
        Tool(
            name="analyze_article_content",
            description="åˆ†æå·²çˆ¬å–çš„æ–‡ç« å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯",
            inputSchema={
                "type": "object",
                "properties": {
                    "article_data": {
                        "type": "object",
                        "description": "æ–‡ç« æ•°æ®å¯¹è±¡"
                    },
                    "analysis_type": {
                        "type": "string",
                        "enum": ["summary", "keywords", "images", "full"],
                        "description": "åˆ†æç±»å‹ï¼šsummary(æ‘˜è¦), keywords(å…³é”®è¯), images(å›¾ç‰‡ä¿¡æ¯), full(å®Œæ•´åˆ†æ)",
                        "default": "full"
                    }
                },
                "required": ["article_data"]
            }
        ),
        Tool(
            name="get_article_statistics",
            description="è·å–æ–‡ç« ç»Ÿè®¡ä¿¡æ¯ï¼ˆå­—æ•°ã€å›¾ç‰‡æ•°é‡ç­‰ï¼‰",
            inputSchema={
                "type": "object",
                "properties": {
                    "article_data": {
                        "type": "object",
                        "description": "æ–‡ç« æ•°æ®å¯¹è±¡"
                    }
                },
                "required": ["article_data"]
            }
        )
    ]


@server.call_tool()
async def handle_call_tool(name: str, arguments: Dict[str, Any]) -> List[TextContent]:
    """å¤„ç†å·¥å…·è°ƒç”¨"""
    try:
        if name == "crawl_weixin_article":
            return await crawl_weixin_article(arguments)
        elif name == "analyze_article_content":
            return await analyze_article_content(arguments)
        elif name == "get_article_statistics":
            return await get_article_statistics(arguments)
        else:
            raise ValueError(f"æœªçŸ¥çš„å·¥å…·: {name}")
    except Exception as e:
        logger.error(f"å·¥å…·è°ƒç”¨å¤±è´¥ {name}: {e}")
        return [TextContent(type="text", text=f"é”™è¯¯: {str(e)}")]


def validate_crawl_parameters(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """éªŒè¯å’Œæ ‡å‡†åŒ–çˆ¬å–å‚æ•°"""
    url = arguments.get("url")
    if not url:
        raise ValueError("ç¼ºå°‘å¿…éœ€å‚æ•°: url")
    
    if not isinstance(url, str) or not url.startswith("https://mp.weixin.qq.com/"):
        raise ValueError("æ— æ•ˆçš„å¾®ä¿¡æ–‡ç« URLï¼Œå¿…é¡»ä»¥ https://mp.weixin.qq.com/ å¼€å¤´")
    
    # è®¾ç½®é»˜è®¤å€¼å¹¶éªŒè¯
    validated = {
        "url": url,
        "download_images": arguments.get("download_images", True),  # é»˜è®¤ä¸ºTrue
        "custom_filename": arguments.get("custom_filename")
    }
    
    logger.info(f"å‚æ•°éªŒè¯é€šè¿‡: download_images={validated['download_images']}, url={url[:50]}...")
    return validated

async def crawl_weixin_article(arguments: Dict[str, Any]) -> List[TextContent]:
    """çˆ¬å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« """
    try:
        # éªŒè¯å’Œæ ‡å‡†åŒ–å‚æ•°
        validated_args = validate_crawl_parameters(arguments)
        url = validated_args["url"]
        download_images = validated_args["download_images"]
        custom_filename = validated_args["custom_filename"]
        
    except ValueError as e:
        return [TextContent(type="text", text=f"å‚æ•°é”™è¯¯: {str(e)}")]
    
    try:
        # è·å–çˆ¬è™«å®ä¾‹
        spider = get_spider_instance()
        
        # è®¾ç½®æ˜¯å¦ä¸‹è½½å›¾ç‰‡
        spider.download_images = download_images
        
        logger.info(f"å¼€å§‹çˆ¬å–æ–‡ç« : {url}")
        
        # çˆ¬å–æ–‡ç« 
        article_data = spider.crawl_article_by_url(url)
        
        if not article_data:
            return [TextContent(type="text", text="çˆ¬å–å¤±è´¥: æ— æ³•è·å–æ–‡ç« å†…å®¹")]
        
        # ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶
        success = spider.save_article_to_file(article_data, custom_filename)
        
        if success:
            # æ„å»ºè¿”å›ç»“æœ
            result = {
                "status": "success",
                "message": "æ–‡ç« çˆ¬å–æˆåŠŸ",
                "article": {
                    "title": article_data.get("title", ""),
                    "author": article_data.get("author", ""),
                    "publish_time": article_data.get("publish_time", ""),
                    "url": article_data.get("url", ""),
                    "content_length": len(article_data.get("content", "")),
                    "images_count": len(article_data.get("images", [])),
                    "crawl_time": article_data.get("crawl_time", "")
                },
                "files_saved": {
                    "json": True,
                    "txt": True,
                    "images": download_images
                }
            }
            
            if download_images:
                images = article_data.get("images", [])
                success_count = sum(1 for img in images if img.get("download_success", False))
                result["article"]["images_downloaded"] = f"{success_count}/{len(images)}"
            
            return [TextContent(
                type="text", 
                text=f"âœ… æ–‡ç« çˆ¬å–æˆåŠŸ\n\n{json.dumps(result, ensure_ascii=False, indent=2)}"
            )]
        else:
            return [TextContent(type="text", text="çˆ¬å–å¤±è´¥: ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™")]
            
    except Exception as e:
        logger.error(f"çˆ¬å–æ–‡ç« å¤±è´¥: {e}")
        return [TextContent(type="text", text=f"çˆ¬å–å¤±è´¥: {str(e)}")]


def validate_analysis_parameters(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """éªŒè¯åˆ†æå‚æ•°"""
    article_data = arguments.get("article_data")
    if not article_data:
        raise ValueError("ç¼ºå°‘å¿…éœ€å‚æ•°: article_dataã€‚è¯·å…ˆä½¿ç”¨ crawl_weixin_article å·¥å…·çˆ¬å–æ–‡ç« æ•°æ®")
    
    if not isinstance(article_data, dict):
        raise ValueError("article_data å¿…é¡»æ˜¯å­—å…¸æ ¼å¼çš„æ–‡ç« æ•°æ®")
    
    # æ£€æŸ¥æ–‡ç« æ•°æ®çš„åŸºæœ¬å­—æ®µ
    required_fields = ["title", "content"]
    missing_fields = [field for field in required_fields if field not in article_data]
    if missing_fields:
        logger.warning(f"æ–‡ç« æ•°æ®ç¼ºå°‘å­—æ®µ: {missing_fields}")
    
    validated = {
        "article_data": article_data,
        "analysis_type": arguments.get("analysis_type", "full")
    }
    
    logger.info(f"åˆ†æå‚æ•°éªŒè¯é€šè¿‡: analysis_type={validated['analysis_type']}, æ–‡ç« æ ‡é¢˜={article_data.get('title', 'N/A')[:30]}...")
    return validated

async def analyze_article_content(arguments: Dict[str, Any]) -> List[TextContent]:
    """åˆ†ææ–‡ç« å†…å®¹"""
    try:
        # éªŒè¯å‚æ•°
        validated_args = validate_analysis_parameters(arguments)
        article_data = validated_args["article_data"]
        analysis_type = validated_args["analysis_type"]
        
    except ValueError as e:
        return [TextContent(type="text", text=f"å‚æ•°é”™è¯¯: {str(e)}")]
    
    try:
        result = {"analysis_type": analysis_type}
        
        if analysis_type in ["summary", "full"]:
            content = article_data.get("content", "")
            result["summary"] = {
                "title": article_data.get("title", ""),
                "author": article_data.get("author", ""),
                "publish_time": article_data.get("publish_time", ""),
                "content_preview": content[:200] + "..." if len(content) > 200 else content,
                "word_count": len(content),
                "paragraph_count": len(content.split("\n\n")) if content else 0
            }
        
        if analysis_type in ["keywords", "full"]:
            content = article_data.get("content", "")
            # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå¯ä»¥åç»­é›†æˆæ›´å¤æ‚çš„NLPåº“ï¼‰
            words = content.split()
            word_freq = {}
            for word in words:
                if len(word) > 1:  # è¿‡æ»¤å•å­—ç¬¦
                    word_freq[word] = word_freq.get(word, 0) + 1
            
            # è·å–å‰10ä¸ªé«˜é¢‘è¯
            top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
            result["keywords"] = [word for word, freq in top_words]
        
        if analysis_type in ["images", "full"]:
            images = article_data.get("images", [])
            result["images_analysis"] = {
                "total_count": len(images),
                "downloaded_count": sum(1 for img in images if img.get("download_success", False)),
                "failed_count": sum(1 for img in images if not img.get("download_success", False)),
                "image_details": [
                    {
                        "filename": img.get("filename", ""),
                        "alt_text": img.get("alt", ""),
                        "download_success": img.get("download_success", False)
                    }
                    for img in images[:5]  # åªæ˜¾ç¤ºå‰5å¼ å›¾ç‰‡çš„è¯¦æƒ…
                ]
            }
        
        return [TextContent(
            type="text",
            text=f"ğŸ“Š æ–‡ç« åˆ†æç»“æœ\n\n{json.dumps(result, ensure_ascii=False, indent=2)}"
        )]
        
    except Exception as e:
        logger.error(f"åˆ†ææ–‡ç« å†…å®¹å¤±è´¥: {e}")
        return [TextContent(type="text", text=f"åˆ†æå¤±è´¥: {str(e)}")]


async def get_article_statistics(arguments: Dict[str, Any]) -> List[TextContent]:
    """è·å–æ–‡ç« ç»Ÿè®¡ä¿¡æ¯"""
    article_data = arguments.get("article_data")
    
    if not article_data:
        return [TextContent(type="text", text="é”™è¯¯: ç¼ºå°‘æ–‡ç« æ•°æ®")]
    
    try:
        content = article_data.get("content", "")
        images = article_data.get("images", [])
        
        stats = {
            "basic_info": {
                "title": article_data.get("title", ""),
                "author": article_data.get("author", ""),
                "publish_time": article_data.get("publish_time", ""),
                "crawl_time": article_data.get("crawl_time", "")
            },
            "content_statistics": {
                "total_characters": len(content),
                "total_words": len(content.split()),
                "paragraphs": len(content.split("\n\n")) if content else 0,
                "lines": len(content.split("\n")) if content else 0
            },
            "image_statistics": {
                "total_images": len(images),
                "downloaded_successfully": sum(1 for img in images if img.get("download_success", False)),
                "download_failed": sum(1 for img in images if not img.get("download_success", False)),
                "download_success_rate": f"{(sum(1 for img in images if img.get('download_success', False)) / len(images) * 100):.1f}%" if images else "0%"
            }
        }
        
        return [TextContent(
            type="text",
            text=f"ğŸ“ˆ æ–‡ç« ç»Ÿè®¡ä¿¡æ¯\n\n{json.dumps(stats, ensure_ascii=False, indent=2)}"
        )]
        
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return [TextContent(type="text", text=f"ç»Ÿè®¡å¤±è´¥: {str(e)}")]


@server.list_resources()
async def handle_list_resources() -> List[Resource]:
    """åˆ—å‡ºå¯ç”¨çš„èµ„æº"""
    return [
        Resource(
            uri="weixin://articles/recent",
            name="æœ€è¿‘çˆ¬å–çš„æ–‡ç« ",
            description="æ˜¾ç¤ºæœ€è¿‘çˆ¬å–çš„å¾®ä¿¡æ–‡ç« åˆ—è¡¨",
            mimeType="application/json"
        ),
        Resource(
            uri="weixin://config/spider",
            name="çˆ¬è™«é…ç½®",
            description="å½“å‰çˆ¬è™«çš„é…ç½®ä¿¡æ¯",
            mimeType="application/json"
        )
    ]


def find_articles_directory() -> Path:
    """æ™ºèƒ½æŸ¥æ‰¾æ–‡ç« ç›®å½•ï¼Œæ”¯æŒå¤šä¸ªå¯èƒ½çš„è·¯å¾„"""
    base_path = Path(__file__).parent.parent.parent
    possible_paths = [
        base_path / "articles",
        base_path / "weixin_articles", 
        base_path / "pythonSpider" / "articles",
        Path("articles"),
        Path("weixin_articles")
    ]
    
    for path in possible_paths:
        if path.exists() and path.is_dir():
            logger.info(f"æ‰¾åˆ°æ–‡ç« ç›®å½•: {path}")
            return path
    
    logger.warning(f"æœªæ‰¾åˆ°æ–‡ç« ç›®å½•ï¼Œå°è¯•çš„è·¯å¾„: {[str(p) for p in possible_paths]}")
    return base_path / "articles"  # è¿”å›é»˜è®¤è·¯å¾„

@server.read_resource()
async def handle_read_resource(uri: str) -> str:
    """è¯»å–èµ„æºå†…å®¹ - æ”¯æŒæ™ºèƒ½è·¯å¾„æŸ¥æ‰¾"""
    try:
        if uri == "weixin://articles/recent":
            # æ™ºèƒ½æŸ¥æ‰¾æ–‡ç« ç›®å½•
            articles_dir = find_articles_directory()
            
            if articles_dir.exists():
                recent_articles = []
                for article_dir in sorted(articles_dir.iterdir(), key=lambda x: x.stat().st_mtime, reverse=True)[:10]:
                    if article_dir.is_dir():
                        json_file = article_dir / f"{article_dir.name}.json"
                        if json_file.exists():
                            try:
                                with open(json_file, 'r', encoding='utf-8') as f:
                                    article_data = json.load(f)
                                recent_articles.append({
                                    "title": article_data.get("title", ""),
                                    "author": article_data.get("author", ""),
                                    "crawl_time": article_data.get("crawl_time", ""),
                                    "directory": str(article_dir)
                                })
                            except Exception as e:
                                logger.error(f"è¯»å–æ–‡ç« æ–‡ä»¶å¤±è´¥ {json_file}: {e}")
                
                return json.dumps({
                    "recent_articles": recent_articles,
                    "articles_directory": str(articles_dir),
                    "total_found": len(recent_articles)
                }, ensure_ascii=False, indent=2)
            else:
                return json.dumps({
                    "recent_articles": [], 
                    "message": "æ–‡ç« ç›®å½•ä¸å­˜åœ¨",
                    "searched_paths": [str(p) for p in [articles_dir]]
                }, ensure_ascii=False)
        
        elif uri == "weixin://config/spider":
            config = {
                "spider_status": "ready" if spider_instance else "not_initialized",
                "default_settings": {
                    "headless": True,
                    "wait_time": 10,
                    "download_images": True
                },
                "supported_features": [
                    "æ–‡ç« å†…å®¹æŠ“å–",
                    "å›¾ç‰‡ä¸‹è½½",
                    "å¤šæ ¼å¼ä¿å­˜ (JSON, TXT)",
                    "å†…å®¹åˆ†æ",
                    "ç»Ÿè®¡ä¿¡æ¯"
                ]
            }
            return json.dumps(config, ensure_ascii=False, indent=2)
        
        else:
            raise ValueError(f"æœªçŸ¥çš„èµ„æºURI: {uri}")
            
    except Exception as e:
        logger.error(f"è¯»å–èµ„æºå¤±è´¥ {uri}: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


async def cleanup():
    """æ¸…ç†èµ„æº"""
    global spider_instance
    if spider_instance:
        try:
            spider_instance.close()
            logger.info("çˆ¬è™«å®ä¾‹å·²å…³é—­")
        except Exception as e:
            logger.error(f"å…³é—­çˆ¬è™«å®ä¾‹å¤±è´¥: {e}")
        finally:
            spider_instance = None


async def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ£€æŸ¥çˆ¬è™«æ¨¡å—æ˜¯å¦å¯ç”¨
        if WeixinSpiderWithImages is None:
            logger.warning("çˆ¬è™«æ¨¡å—æœªå¯¼å…¥ï¼ŒæœåŠ¡å™¨å°†ä»¥æœ‰é™åŠŸèƒ½è¿è¡Œ")
        else:
            logger.info("çˆ¬è™«æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # å¯åŠ¨MCPæœåŠ¡å™¨
        async with stdio_server() as (read_stream, write_stream):
            logger.info("MCPå¾®ä¿¡çˆ¬è™«æœåŠ¡å™¨å¯åŠ¨")
            try:
                await server.run(
                    read_stream,
                    write_stream,
                    InitializationOptions(
                        server_name="mcp-weixin-spider",
                        server_version="0.1.0",
                        capabilities=ServerCapabilities(
                            tools={},
                            resources={}
                        )
                    )
                )
            except Exception as e:
                logger.error(f"æœåŠ¡å™¨è¿è¡Œæ—¶é”™è¯¯: {e}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                raise
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡å™¨...")
    except Exception as e:
        logger.error(f"æœåŠ¡å™¨å¯åŠ¨é”™è¯¯: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        raise
    finally:
        await cleanup()
        logger.info("MCPå¾®ä¿¡çˆ¬è™«æœåŠ¡å™¨å·²å…³é—­")


if __name__ == "__main__":
    asyncio.run(main())